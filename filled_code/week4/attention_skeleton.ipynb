{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CfPUWThiVrZ"
   },
   "source": [
    "# Attention\n",
    "\n",
    "### Recommended Material:\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=eMlx5fFNoYc\">Great Attention Visual Explainer Video by 3B1B</a> (Highly Recommended)\n",
    "\n",
    "<a href=\"https://nlp.seas.harvard.edu/annotated-transformer/#:~:text=interactive()%0A%20%20%20%20)%0A%0A%0Ashow_example(example_mask)-,Attention,-An%20attention%20function\">Attention Section from \"The Annotated Transformer\"</a>\n",
    "\n",
    "## Motivation for Attention\n",
    "Context is really important. Many words can take on very different meaning depending on the contexts they appear in.\n",
    "\n",
    "For example the word \"bar\" could refer to\n",
    "- the place where you get drinks\n",
    "- a long rod of material\n",
    "- a measure of music\n",
    "- the law exam\n",
    "\n",
    "the list goes on.\n",
    "\n",
    "With word embeddings alone, regardless of the context, or the other tokens in the sequence, the string \"bar\" will be mapped to the same word embedding despite having many unrelated meanings.\n",
    "\n",
    "In order to determine what definition bar refers to we need to be able to gather information from the other words in the text.\n",
    "\n",
    "Another example where this is useful is pronouns, since there needs to be some way to determine which proper noun is being referred to.\n",
    "\n",
    "In general, the model would benefit from some way to share information between tokens, to add context.\n",
    "\n",
    "Since all the information about a token is stored in that token's high-dimensional vector (initially just the word embedding + positional embedding), we want to add/subtract to this token's vector based on each token around it to store this additional information. The 3B1B video has some nice visual intuition for this.\n",
    "\n",
    "## Big Idea\n",
    "In a nutshell, the self-attention mechanism allows each token to look backwards at the tokens that come before it and add information to it's vector by adding or subtracting.\n",
    "\n",
    "It can be thought of as the following steps:\n",
    "\n",
    "Each token emits a **Query** vector that roughly represents \"What am I looking for?\"\n",
    "\n",
    "Each token also emits a **Key** vector that roughly represents \"What am I in the context of this attention head?\" or \"What is my answer to the query?\"\n",
    "\n",
    "Each token also emits a **Value** vector that roughly represents \"If someone finds me to be a match (my key matches their query), what information should I give to them / add to their vector?\"\n",
    "\n",
    "## Details\n",
    "\n",
    "### Obtaining Q, K, V\n",
    "The way that the attention mechanism computes the Query, Key, and Value vectors is that each token (the vector) is passed through 3 separate linear layers in in parallel. The weights and biases for these layers are parameters learned by the model. For example to get the query, Q = W_Q * residual + b_Q. (The vector dimensions go from d_model to d_head, sometimes in multiheaded attention the model dimension is broken up between each attention head)\n",
    "\n",
    "In practice, all the vectors are passed in at once with some matrix multiplication.\n",
    "\n",
    "### Attending to other tokens\n",
    "\n",
    "Once we obtain these matrices containing the queries, keys, and values of each token. We want to take the dot product of the queries and keys to get a vector for each token that represents how much the other tokens' keys \"answered\"/matched my query.\n",
    "\n",
    "The dot product will be higher between some query and key vector when that query and key vector are aligned. Sometimes this is referred to as the token with the query \"attending\" to the other token with an aligned key.\n",
    "\n",
    "#### Rescaling\n",
    "\n",
    "We rescale the attention matrix by dividing it by the square root of d_head or d_k, a.k.a. the head dimension.\n",
    "\n",
    "#### Causal Masking\n",
    "\n",
    "We apply a causal mask such that only the upper triangular values of the matrix are left. In self-attention we don't want any token to attend to tokens in front of it, because during training we have the model predict the next token for each token in parallel. Seeing forward, would allow the model to know the ground-truth answer.\n",
    "\n",
    "Since we apply this mask before softmaxing, for the values we want to remove we will fill them with negative infinity so that after softmaxing, it will be 0.\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "We want the attention values for a particular token to add up to 1, so we will apply softmax.\n",
    "\n",
    "#### Dotting with Values\n",
    "\n",
    "Then to find out what we should add, we take the dot product of this with the Value matrix such that the attention score with the other token is dotted with the value of the other token and these are summed up and added to each token.\n",
    "\n",
    "$$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "After this, the values are passed through another linear layer and in the case of multiheaded attention, the outputs are added across the heads and finally added back onto the residual stream.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/transformer-attn-21.png\" width=\"1400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "q3qT_hb_114-"
   },
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PYI5dfB2Rmr"
   },
   "source": [
    "### References for Einops Einsum\n",
    "https://einops.rocks/api/einsum/\n",
    "\n",
    "Understanding Einsum in general (video uses np):\n",
    "https://www.youtube.com/watch?v=pkVwUVEHmfI\n",
    "\n",
    "`einsum` and `einops` streamline tensor operations in attention mechanisms by eliminating the need for repetitive and error-prone tensor transpositions and reshaping. Specifically:\n",
    "\n",
    "`einsum`: Enables concise, readable, and efficient tensor contractions and summations across multiple dimensions by specifying how indices are related. This is particularly useful in attention mechanisms for computing dot products, such as the query-key dot product, without manual transpositions.\n",
    "\n",
    "You can find an implementation without einops in Karpathy's GPT from scratch but you would basically do the transposing and then dot products rather than being able to notate it the two operations in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8JkN9wy4ogN"
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell",
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "def hidden_apply_causal_mask(\n",
    "    attn_scores: Tensor, masked_value: float = float(\"-inf\")\n",
    ") -> Tensor:\n",
    "    # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "    all_ones = torch.ones(\n",
    "        attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device\n",
    "    )\n",
    "    mask = torch.triu(all_ones, diagonal=1).bool()\n",
    "    # Apply the mask to attention scores, then return the masked scores\n",
    "    attn_scores.masked_fill_(mask, masked_value)\n",
    "\n",
    "    return attn_scores\n",
    "\n",
    "\n",
    "class HiddenAttention(nn.Module):\n",
    "    def __init__(self, num_heads: Tensor, dim_model: Tensor, dim_head: Tensor) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # hyper parameters\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "        # weights\n",
    "        self.W_Q = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
    "        self.W_K = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
    "        self.W_V = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
    "        self.W_O = nn.Parameter(torch.ones((num_heads, dim_head, dim_model)))\n",
    "\n",
    "        # biases\n",
    "        self.b_Q = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
    "        self.b_K = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
    "        self.b_V = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
    "        self.b_O = nn.Parameter(torch.zeros((dim_model)))\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass of the attention layer.\n",
    "    Takes a tensor of shape [batch, tokens, dim_model]\n",
    "    Outputs a tensor of shape [batch, tokens, dim_model]\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        tokens_size = x.shape[1]\n",
    "\n",
    "        # Calculate query, key and value vectors\n",
    "        q = (\n",
    "            einops.einsum(\n",
    "                x,\n",
    "                self.W_Q,\n",
    "                \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        assert q.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "        k = (\n",
    "            einops.einsum(\n",
    "                x,\n",
    "                self.W_K,\n",
    "                \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        assert k.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "        v = (\n",
    "            einops.einsum(\n",
    "                x,\n",
    "                self.W_V,\n",
    "                \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\",\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "        assert v.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q,\n",
    "            k,\n",
    "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\",\n",
    "        )\n",
    "        assert attn_scores.shape == torch.Size(\n",
    "            [batch_size, self.num_heads, tokens_size, tokens_size]\n",
    "        )\n",
    "        attn_scores_masked = hidden_apply_causal_mask(\n",
    "            attn_scores / self.dim_head**0.5, float(\"-inf\")\n",
    "        )\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v,\n",
    "            attn_pattern,\n",
    "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\",\n",
    "        )\n",
    "        assert z.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = (\n",
    "            einops.einsum(\n",
    "                z,\n",
    "                self.W_O,\n",
    "                \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\",\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "        assert attn_out.shape == torch.Size([batch_size, tokens_size, self.dim_model])\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Applies a causal mask to attention scores, and returns masked scores.\n",
    "Takes an input of size [batch, n_heads, query_pos, key_pos]\n",
    "And outputs a tensor of size [batch, n_heads, query_pos, key_pos]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_causal_mask(\n",
    "    attn_scores: Tensor, masked_value: float = float(\"-inf\")\n",
    ") -> Tensor:\n",
    "    # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "    mask = None\n",
    "    # Apply the mask to attention scores and replace the masked values with the ignore value masked_value\n",
    "    masked_attn_scores = None\n",
    "\n",
    "    return masked_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "allclose(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m ignore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m test1 \u001b[38;5;241m=\u001b[39m apply_causal_mask(torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m      6\u001b[0m     [\u001b[38;5;241m1.\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m      7\u001b[0m     [\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m],\n\u001b[1;32m      8\u001b[0m     [\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m],\n\u001b[1;32m      9\u001b[0m ]), ignore)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOh no it looks like your matrix doesnt pass test 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(test1)\n",
      "\u001b[0;31mTypeError\u001b[0m: allclose(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test case for your apply_causal_mask\n",
    "\"\"\"\n",
    "ignore = float(\"-inf\")\n",
    "test1 = apply_causal_mask(\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [1.0, 2, 3],\n",
    "            [4, 5, 6],\n",
    "            [7, 8, 9],\n",
    "        ]\n",
    "    ),\n",
    "    ignore,\n",
    ")\n",
    "\n",
    "assert torch.allclose(\n",
    "    test1, torch.tensor([[1.0, ignore, ignore], [4.0, 5.0, ignore], [7.0, 8.0, 9.0]])\n",
    "), \"Oh no it looks like your matrix doesnt pass test 1\"\n",
    "print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, num_heads: Tensor, dim_model: Tensor, dim_head: Tensor) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # hyper parameters\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "        # weights\n",
    "        self.W_Q = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
    "        self.W_K = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
    "        self.W_V = nn.Parameter(torch.ones((num_heads, dim_model, dim_head)))\n",
    "        self.W_O = nn.Parameter(torch.ones((num_heads, dim_head, dim_model)))\n",
    "\n",
    "        # biases\n",
    "        self.b_Q = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
    "        self.b_K = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
    "        self.b_V = nn.Parameter(torch.zeros((num_heads, dim_head)))\n",
    "        self.b_O = nn.Parameter(torch.zeros((dim_model)))\n",
    "\n",
    "    \"\"\"\n",
    "    Forward pass of the attention layer.\n",
    "    Takes a tensor of shape [batch, tokens, dim_model]\n",
    "    Outputs a tensor of shape [batch, tokens, dim_model]\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "        tokens_size = x.shape[1]\n",
    "\n",
    "        # Calculate query, key and value vectors\n",
    "        q = None\n",
    "        assert q.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "        k = None\n",
    "        assert k.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "        v = None\n",
    "        assert v.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = None\n",
    "        assert attn_scores.shape == torch.Size(\n",
    "            [batch_size, self.num_heads, tokens_size, tokens_size]\n",
    "        )\n",
    "        attn_scores_masked = None\n",
    "        attn_probs = None\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = None\n",
    "        assert z.shape == torch.Size(\n",
    "            [batch_size, tokens_size, self.num_heads, self.dim_head]\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = None\n",
    "        assert attn_out.shape == torch.Size([batch_size, tokens_size, self.dim_model])\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "tokens_dim = 20\n",
    "dim_model = 30\n",
    "dim_heads = 10\n",
    "num_heads = 2\n",
    "ground_truth = HiddenAttention(num_heads, dim_model, dim_heads)\n",
    "user_model = Attention(num_heads, dim_model, dim_heads)\n",
    "test = torch.rand((batch_size, tokens_dim, dim_model))\n",
    "\n",
    "truth_output = ground_truth(test)\n",
    "user_output = user_model(test)\n",
    "\n",
    "assert torch.allclose(\n",
    "    truth_output, user_output\n",
    "), \"Uh oh your model doesn't give the same outputs\"\n",
    "print(\"passed all tests!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
