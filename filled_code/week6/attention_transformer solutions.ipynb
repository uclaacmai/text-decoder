{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to create an attention mechanism, we will use it in this notebook to implement an attention transformer in PyTorch. The format of the notebook will be roughly the same as previous notebooks (just fill in when it asks for your_implementation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to import the PyTorch libraries to build the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "# Can import from attention module in week4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the hyperparameters for the model and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000 # how many iterations do we want to train our model for\n",
    "eval_interval = 100 # at which iterations do we perform our evaluation\n",
    "learning_rate = 1e-3 # how much do we want to optimize our weights at each step\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # determines device we run the tensor on\n",
    "eval_iters = 200 # how many evaluation intervals do we use to get the loss average\n",
    "n_embd = 64 # dimension of embeddings for our input\n",
    "n_head = 4 # number of attention heads working in parallel\n",
    "n_layer = 4 # number of layers in our attention head that our input goes through\n",
    "dropout = 0.0 # dropout probability aka probability that a weight turns to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in our dataset to be used by the Transformer Model we are creating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing Data\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are miscellaneous functions that will be used in our model training and evaluation. \n",
    "\n",
    "The first function gets the batches for out dataset. For our data, we often process it in batches. Each batch is a set size of data taken randomly from the dataset. Each training epoch operates on one batch. The input is always what is ahead in the sequence and the target is the next character.\n",
    "\n",
    "The second function estimates the loss for our model using a simple training loop with gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Miscellaneous Functions\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    # estimate the average loss for each data split for evaluation\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build LayerNorm from scratch. \n",
    "\n",
    "![The equation we use for LayerNorm](https://miro.medium.com/v2/resize:fit:1040/0*qN-QGSHiY85obQfj)\n",
    "\n",
    "Basically, LayerNorm takes the mean and variance of the input x and uses it to normalize the inputs. Following the Central Limit Theorem, we normalize the inputs so that they have a mean of 0 and a variance of 1 by subtracting the mean and dividing by the standard deviation (square root of variance). This allows the inputs to follow a standard normal distribution preventing the data from getting extraneous or outlier values that can cause exploding or vanishing gradients, allowing for more stable training. In order to make sure that we don't divide by 0, we add epsilon, a small value, to the denominator.\n",
    "\n",
    "Next, we have the shift and scale parameters. Gamma is the scale parameter which is basically the variance of the normalized distribution. You can think of it as how much the normal distribution is stretched from the standard normal (like how large the range of common values is). Beta is the shift parameter which is basically the mean of the normalized distribution. This is where the normal distribution is centered. These two parameters are learned so you can adjust them to the data that is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # initialize the parameters of the LayerNorm equation\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # implement LayerNorm based on the equation above\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FeedFoward(nn.Module):\n",
    "#     \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "#     def __init__(self, n_embd):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(n_embd, 4 * n_embd),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(4 * n_embd, n_embd),\n",
    "#             nn.Dropout(dropout),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets build the attention mechanism for our Transformer. This is the core component that makes Transformers so powerful.\n",
    "\n",
    "First, we will define the attention head where our attention mechanism will take place. In it we initialize the key, query, and value projection layers where we project the inputs into another subspace (think of it as another dimension or embedding of storing information on the input semantics). These projection layers are learned and can be adjusted. We also defined something called \"tril\" which we will go into more in just a second. Finally, there is dropout a regularization technique to prevent overfitting and allow the model to learn more effectively.\n",
    "\n",
    "In the attention mechanism, we first need to get our key, query, and value embeddings. We do this by throwing the input through linear layers to map them to another information filled subspace. These embeddings are very important. For example, lets say your input is a sentence. Each sentence is broken up into tokens (typically one word) and these tokens are represented by embeddings. The projection layers map these embeddings into another subspace as vectors (key, value, and query). The magnitude and direction or what defines these vectors encode information about the token they are correlated to. We will use this information later on to determine the meaning of the input sentence in our attention mechanism. \n",
    "\n",
    "We can use Week 4's attention notebook and implementation here.\n",
    "\n",
    "![Here is the attention mechanism equation](https://miro.medium.com/v2/resize:fit:1400/0*4L90D4iDB_R1Uljs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After developing our attention mechanism, the engine of the Transformer model, we need to add a few things to complete the architecture. One of them is MultiHeadAttention. The idea of MultiHeadAttention is to allow for multiple attention mechanisms to happen in parallel allowing for more information and semantic parsing. In longer sequences with longer contexts, this extra information is very useful in order to maximize accuracy and efficiency. Each attention head in a MultiHeadAttention block can learn a separate function and thus retain different things about the input. For example, with a sentence, one head can learn about what the subject is doing and another head can learn about how the subject looks.\n",
    "\n",
    "![Diagram of MultiHeadAttention](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR2H71D22diHDQKf6STcbHbRgvdynJ_c0RZZA&s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        # self.proj = nn.Linear(n_embd, n_embd)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # print(out.shape)\n",
    "        # out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to put it all together into a Transformer Block. First, we initialize the MultiHeadAttention block to be used in the Transformer. For each head, the subspace that they work in is the size of the embedding divided by the number of heads. The more heads can lead to more parallel attention mechanisms at once but also force the embedding space they work in to be smaller, leading to more limited information. We also initialize a layer norn as regularization for the output of the MultiHeadAttention block. In a normal Transformer block there would also be a feed forward but we will cover that in more detail later.\n",
    "\n",
    "The forward function just uses the initialized MultiHeadAttention on the input and throws the output of that throught the LayerNorm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        # initialize the components of a attention only Block\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # put the input throught the intialized components\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigramAttentionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put the blocks together to create the BigramAttentionModel. First we have a token_embedding_table which basically embeds the input tokens into embeddings based on a table. These embeddings are numerical representations of the input, vectors of size n_embd that contain information of the input they are representing. \n",
    "\n",
    "![Here is a diagram of embeddings. As you can see words are mapped to vectors where more related words have more similar vectors](https://arize.com/wp-content/uploads/2022/06/blog-king-queen-embeddings.jpg)\n",
    "\n",
    "Next, we have the position_embedding_table which is used to encode the position of each token in the input. This is another lookup table that takes the position of a token in the sequence and encodes it in a numerical vector of the same size as the input embeddings. This is done so that we can add the positional embeddings to the input embeddings. This allows the model to have awareness of where in a sentence a specific token is and the relative distances between different tokens. \n",
    "\n",
    "![In a typical Transformer, we use sin and cos functions to create positional encodings. However, we are just using a table for this application. Here is how positional encodings are used within a LLM.](https://miro.medium.com/v2/resize:fit:1400/0*oiP-eu8BmJx5SVp7.png)\n",
    "\n",
    "Lastly, we have the actual transformer blocks which is the workhorse of the model. These blocks contain the multi headed attention mechanism. \n",
    "\n",
    "We finally have a linear layer to map the embeddings and the information they carry to the vocab_size so they can be decoded character by character into natural language. LayerNorm is used once again to regularize the output of the blocks. \n",
    "\n",
    "The forward function uses all the initialized modules on the input in order and then if a target is provided, helps compute the loss. We use cross entropy between the predicted next character logit (which is basically the probability distribution of the next character) and the target next character to calculate the loss. In order to make it compatible with the cross entropy function we need to reshape the tensor so that the dimensions match. The dimensions of the two tensors are provided in the comments.\n",
    "\n",
    "We also have a generate function for inference. The idea of this function is to generate max_new_tokens of characters within a natural language passage using the Bigram Language Transformer Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major hyperparameter to consider is depth of the model. To create a deeper model, we can add more Transformer Blocks to the model (increase what n_layer is). But what is the advantage of that?\n",
    "\n",
    "In lots of scenarios, stacking layers and having more parameters can lead to greater results as evidenced by the increasing size of LLMs and their growing capabilities. However, just stacking more blocks isn't necessarily easy.\n",
    "\n",
    "More blocks means more compute and that means more necessary GPUs and physical hardware to support the machine. There's a reason why only big companies with lots of money can create the best models in the world. There's also the problem of unstable training. With bigger models, they become harder to train due to unstable gradients. Gradient calculation starts from the lowest layer and becomes smaller and smaller as you go through the model. This can lead to vanishing gradients or increasingly slow training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        logits = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(logits) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # (B*T, C)\n",
    "            targets = targets.view(B*T) # (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the training module. Here we intialize the model, print the parameters, then have a typical training loop for the Transformer Model with evaluation intervals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.060225 M parameters\n",
      "step 0: train loss 4.3867, val loss 4.3800\n",
      "step 100: train loss 3.0054, val loss 3.0246\n",
      "step 200: train loss 2.6671, val loss 2.6851\n",
      "step 300: train loss 2.5606, val loss 2.5675\n",
      "step 400: train loss 2.4906, val loss 2.5020\n",
      "step 500: train loss 2.4404, val loss 2.4377\n",
      "step 600: train loss 2.4089, val loss 2.4043\n",
      "step 700: train loss 2.3601, val loss 2.3743\n",
      "step 800: train loss 2.3442, val loss 2.3313\n",
      "step 900: train loss 2.3211, val loss 2.3368\n",
      "step 1000: train loss 2.2852, val loss 2.2934\n",
      "step 1100: train loss 2.2754, val loss 2.2840\n",
      "step 1200: train loss 2.2501, val loss 2.2645\n",
      "step 1300: train loss 2.2343, val loss 2.2500\n",
      "step 1400: train loss 2.2156, val loss 2.2359\n",
      "step 1500: train loss 2.2058, val loss 2.2278\n",
      "step 1600: train loss 2.1896, val loss 2.2114\n",
      "step 1700: train loss 2.1615, val loss 2.1906\n",
      "step 1800: train loss 2.1630, val loss 2.1883\n",
      "step 1900: train loss 2.1366, val loss 2.1748\n",
      "step 2000: train loss 2.1314, val loss 2.1739\n",
      "step 2100: train loss 2.1152, val loss 2.1620\n",
      "step 2200: train loss 2.1135, val loss 2.1536\n",
      "step 2300: train loss 2.1033, val loss 2.1409\n",
      "step 2400: train loss 2.0833, val loss 2.1406\n",
      "step 2500: train loss 2.0860, val loss 2.1339\n",
      "step 2600: train loss 2.0758, val loss 2.1227\n",
      "step 2700: train loss 2.0634, val loss 2.1254\n",
      "step 2800: train loss 2.0655, val loss 2.1141\n",
      "step 2900: train loss 2.0573, val loss 2.1094\n",
      "step 3000: train loss 2.0470, val loss 2.1056\n",
      "step 3100: train loss 2.0449, val loss 2.1082\n",
      "step 3200: train loss 2.0268, val loss 2.0883\n",
      "step 3300: train loss 2.0223, val loss 2.0874\n",
      "step 3400: train loss 2.0028, val loss 2.0868\n",
      "step 3500: train loss 2.0148, val loss 2.0786\n",
      "step 3600: train loss 2.0012, val loss 2.0788\n",
      "step 3700: train loss 1.9982, val loss 2.0747\n",
      "step 3800: train loss 2.0027, val loss 2.0770\n",
      "step 3900: train loss 1.9855, val loss 2.0687\n",
      "step 4000: train loss 1.9777, val loss 2.0676\n",
      "step 4100: train loss 1.9872, val loss 2.0749\n",
      "step 4200: train loss 1.9756, val loss 2.0525\n",
      "step 4300: train loss 1.9708, val loss 2.0596\n",
      "step 4400: train loss 1.9672, val loss 2.0500\n",
      "step 4500: train loss 1.9586, val loss 2.0472\n",
      "step 4600: train loss 1.9520, val loss 2.0453\n",
      "step 4700: train loss 1.9527, val loss 2.0451\n",
      "step 4800: train loss 1.9434, val loss 2.0392\n",
      "step 4900: train loss 1.9304, val loss 2.0366\n",
      "step 4999: train loss 1.9347, val loss 2.0390\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    # estimate the average loss for each data split for evaluation\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "### Model Training and Initialization\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the model to generate a shakespeare passage since the dataset we gave the model to train on is shakespeare passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WAll becordy.\n",
      "\n",
      "Siad Sir:\n",
      "Well madisen bobe to takegr-and me?\n",
      "Leants art that usquely.\n",
      "\n",
      "PAUMEXENS:\n",
      "Aendwice my feanstacom oroug\n",
      "Youts, tof is heir this now\n",
      "Whineg is eest hein latistlidiov the and the now theeran of likind teal.\n",
      "His me nervey preperness hew yean'st the nroupetel'd gond:\n",
      "I should thak\n",
      "Aon in on her eiiby own thesourise\n",
      "And hime sto-on\n",
      "\n",
      "ADWITHARD:\n",
      "O hanterup af so;\n",
      "Ang hinke me tey aled\n",
      "At there shee my of.\n",
      "\n",
      "HKING ERY:\n",
      "Now is wards. Wich EDn shovein cour aaranny ire to-chan the!\n",
      "\n",
      "JOUnEET:\n",
      "Pke mary thou come blest so a thense.--\n",
      "KING RICHARD Areand the mest hes the wetcend,\n",
      "Thou cous somy shoutcas lort\n",
      "IEn shouth peakes.\n",
      "\n",
      "LAUCIPTABUS:\n",
      "To to-didpasst\n",
      "Whith in meent some so nestas or fether,\n",
      "Thy, com loovers! I sumbuside he nees oug\n",
      "For thirbeake begrerveinn's an so! with selfore,\n",
      "Whoscellosse's is andcessinge, his I ither porven'end.\n",
      "\n",
      "Centey sevencees: timed the now the summpelip!\n",
      "\n",
      "GLIS what that the but speire ollke, of so hat there, drewatch time to ther foru;\n",
      "you gentoness rone, it we heals,\n",
      "Whet Henkeere wher the is the priedisand, and to wen or wardesse, messe mines seelends\n",
      "Singin in to I there, on word, was\n",
      "Prest bereed his anIn\n",
      "Bet a theing and asterst anyserke it blaient!\n",
      "\n",
      "First GRESS:\n",
      "Hiw magitens.\n",
      "\n",
      "ROMEO:\n",
      "If intter yot hey's lendere tas lult maspler,\n",
      "Beold conot jeas ther not crounth?\n",
      "\n",
      "Shall, master nies his; doust betred then the now cher's beed seece nille,\n",
      "Whicty gaing vear ell hoblend the then, to me with teny, delel!\n",
      "HAURHES:\n",
      "I he, so are holers therens; singess shuke themer, say, as;\n",
      "for but of aver scall:\n",
      "O therenee sinher seell wirish a\n",
      "\n",
      "ENCELIZSTHES:\n",
      "Ny hatherery.\n",
      "\n",
      "\n",
      "LOMUCETIOLBE:\n",
      "From'll narws no hantler as I\n",
      "Ignstiaund courmph, shemoras oonge, hathese have blay noten:\n",
      "But wiflente shis I I\n",
      "heave courcilt, of monge,\n",
      "Aod make here twrere withn' in a shat orrtwer with theroner like he pusilf\n",
      "Whats to thee would then of nest woury promend thy and,sent,\n",
      "But hath and of to, Juresh patce,\n",
      "And with down's foorng me sulmest for henide!\n",
      "\n",
      "\n",
      "MUPE\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the passage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
