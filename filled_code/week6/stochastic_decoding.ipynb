{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/anaconda3/envs/gpt/lib/python3.11/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device() -> str:\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "device = torch.device(get_device())\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/anaconda3/envs/gpt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/leon/anaconda3/envs/gpt/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "def generate_n_tokens(\n",
    "    input_ids: torch.Tensor, n: int, sampling_function: callable\n",
    ") -> torch.Tensor:\n",
    "    generated = input_ids.clone()\n",
    "    for _ in range(n):\n",
    "        with torch.no_grad():\n",
    "            logits = model(generated).logits[:, -1, :]\n",
    "        next_token = sampling_function(logits)\n",
    "        generated = torch.cat([generated, next_token.unsqueeze(-1)], dim=-1)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample vocabulary\n",
    "sample_vocab = [\n",
    "    \"token1\",\n",
    "    \"token2\",\n",
    "    \"token3\",\n",
    "    \"token4\",\n",
    "    \"token5\",\n",
    "    \"token6\",\n",
    "    \"token7\",\n",
    "    \"token8\",\n",
    "    \"token9\",\n",
    "    \"token10\",\n",
    "]\n",
    "vocabulary_size = len(sample_vocab)\n",
    "\n",
    "# Sample logits\n",
    "sample_logits = torch.tensor(\n",
    "    [\n",
    "        [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0],\n",
    "        [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0],\n",
    "        [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0],\n",
    "        [1.0, 1.0, 1.0, 1.0, 10.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Function to convert token indices to vocabulary tokens\n",
    "def indices_to_tokens(indices):\n",
    "    return [sample_vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy Search Results: ['token10', 'token1', 'token1', 'token5']\n"
     ]
    }
   ],
   "source": [
    "from stochastic_decoding import greedy_search\n",
    "\n",
    "# Test greedy search\n",
    "greedy_results = greedy_search(sample_logits)\n",
    "print(\"Greedy Search Results:\", indices_to_tokens(greedy_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy Search should always take the highest value logits in each sequnce, therefore you should get:\n",
    "\n",
    "```python\n",
    "Greedy Search Results: ['token10', 'token1', 'token1', 'token5']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Sampling Results: ['token10', 'token1', 'token1', 'token5']\n",
      "Top-3 Sampling Results: ['token8', 'token1', 'token3', 'token5']\n"
     ]
    }
   ],
   "source": [
    "from stochastic_decoding import top_k_sampling, sample_from_logits\n",
    "\n",
    "# Test top-k sampling\n",
    "k = 1\n",
    "top_k_logits = top_k_sampling(sample_logits, k)\n",
    "top_k_results = sample_from_logits(top_k_logits)\n",
    "print(f\"Top-{k} Sampling Results:\", indices_to_tokens(top_k_results))\n",
    "k = 3\n",
    "top_k_logits = top_k_sampling(sample_logits, k)\n",
    "top_k_results = sample_from_logits(top_k_logits)\n",
    "print(f\"Top-{k} Sampling Results:\", indices_to_tokens(top_k_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a k of 1 top k devolves into greedy hence you should get:\n",
    "\n",
    "```python\n",
    "Top-1 Sampling Results: ['token10', 'token1', 'token1', 'token5']\n",
    "```\n",
    "\n",
    "When k is 3 there will be a little more variation but it will likely be that the first token is 10, second 1, the last is 5, and the third is random. Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-p Sampling Results (p=0.05): ['token10', 'token1', 'token1', 'token5']\n",
      "Top-p Sampling Results (p=0.9): ['token10', 'token1', 'token6', 'token5']\n"
     ]
    }
   ],
   "source": [
    "from stochastic_decoding import top_p_sampling\n",
    "\n",
    "# Test top-p sampling\n",
    "p = 0.05\n",
    "top_p_logits = top_p_sampling(sample_logits, p)\n",
    "top_p_results = sample_from_logits(top_p_logits)\n",
    "print(f\"Top-p Sampling Results (p={p}):\", indices_to_tokens(top_p_results))\n",
    "p = 0.9\n",
    "top_p_logits = top_p_sampling(sample_logits, p)\n",
    "top_p_results = sample_from_logits(top_p_logits)\n",
    "print(f\"Top-p Sampling Results (p={p}):\", indices_to_tokens(top_p_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first example we sample the top 5% of logits, since there are only 10 this gives us the top 1 logit, which means that we basically have reduced this to a greedy search (note this isn't true for the last token since it all has equal probability), so I got:\n",
    "```python\n",
    "Top-p Sampling Results (p=0.1): ['token10', 'token1', 'token1', 'token5']\n",
    "```\n",
    "In the second example we take the top 90% of logits, thus we remove one logit from the pool and sample from the remaning so your output will vary but it should have the first token is 10, second is 1, fourth is 5 and, the third is random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Sampling Results (T=0.1): ['token10', 'token1', 'token8', 'token5']\n",
      "Temperature Sampling Results (T=5): ['token5', 'token1', 'token8', 'token4']\n"
     ]
    }
   ],
   "source": [
    "from stochastic_decoding import temperature_sampling\n",
    "\n",
    "# Test temperature sampling\n",
    "temperature = 0.1\n",
    "temp_logits = temperature_sampling(sample_logits, temperature)\n",
    "temp_results = sample_from_logits(temp_logits)\n",
    "print(\n",
    "    f\"Temperature Sampling Results (T={temperature}):\", indices_to_tokens(temp_results)\n",
    ")\n",
    "temperature = 5\n",
    "temp_logits = temperature_sampling(sample_logits, temperature)\n",
    "temp_results = sample_from_logits(temp_logits)\n",
    "print(\n",
    "    f\"Temperature Sampling Results (T={temperature}):\", indices_to_tokens(temp_results)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a temprature value of less than 1 makes the highest probability logit increase in probability and reduces the rest, at a very small temprature it degenerates into a greedy search. Thus you should get the the first, second, and fourth token are the same as greedy. Note that since all logits for the third token have equal probability it will give a random logit for it.\n",
    "\n",
    "```python\n",
    "Temperature Sampling Results (T=0.1): ['token10', 'token1', 'token5', 'token5']\n",
    "```\n",
    "\n",
    "Note that since a temprature greater than 1 flattens the disribution all tokens become more likely so its a bit more random (this is sometimes referred to as the \"creativity\" of the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: Once upon a time, there was a man who was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power\n",
      "Top-k: Once upon a time, there was a time where I was a little bit of a recluse. I would go into an apartment and sit at a desk and read, and then, as I read, I would sit at my desk and\n",
      "Top-p: Once upon a time, there was a great deal of talk about the future of the game.\n",
      "\n",
      "\"I think we're going to have a great year,\" he said. \"We're going to have a great year. We're\n",
      "Temperature: Once upon a time, there was a face-saving player ploy executed politically against Latin American pop factories terrified to expand their American impact Indonesia fuelled havocess ones pockets, increased DH Anne Universal Washington translates resin to to undergo well designed ads laced with\n"
     ]
    }
   ],
   "source": [
    "# Generate n tokens using different sampling strategies\n",
    "n_tokens = 40\n",
    "\n",
    "# Prepare input\n",
    "text = \"Once upon a time, there was a\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "greedy_output = generate_n_tokens(input_ids, n_tokens, greedy_search)\n",
    "top_k_output = generate_n_tokens(\n",
    "    input_ids, n_tokens, lambda x: sample_from_logits(top_k_sampling(x, k=5))\n",
    ")\n",
    "top_p_output = generate_n_tokens(\n",
    "    input_ids, n_tokens, lambda x: sample_from_logits(top_p_sampling(x, p=0.05))\n",
    ")\n",
    "temp_output = generate_n_tokens(\n",
    "    input_ids,\n",
    "    n_tokens,\n",
    "    lambda x: sample_from_logits(temperature_sampling(x, temperature=1.5)),\n",
    ")\n",
    "\n",
    "# Decode outputs\n",
    "print(\"Greedy:\", tokenizer.decode(greedy_output[0], clean_up_tokenization_spaces=True))\n",
    "print(\"Top-k:\", tokenizer.decode(top_k_output[0], clean_up_tokenization_spaces=True))\n",
    "print(\"Top-p:\", tokenizer.decode(top_p_output[0], clean_up_tokenization_spaces=True))\n",
    "print(\n",
    "    \"Temperature:\", tokenizer.decode(temp_output[0], clean_up_tokenization_spaces=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with greedy is that it tends to get stuck in a loop, for instance I got:\n",
    "\n",
    "> Greedy: Once upon a time, there was a man who was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power\n",
    "\n",
    "If your top k is too restrictive (low) you end up haveing very minimal variety (notice that we set it to 5) so we end up with a lot of repitition of ideas and sometimes it gets stuck in a loop:\n",
    "\n",
    "> Top-k: Once upon a time, there was a certain amount of excitement. It was like the moment you're going to get a new car, you're going to have an opportunity to see the car. And you're going to be able to see\n",
    "\n",
    "If your top p is too low you get the same problem as with top k above.\n",
    "\n",
    "> Top-p: Once upon a time, there was a man who was a member of the Church of England, and who had been a member of the Church of England for a long time. He was a man of great faith, and of great integrity.\n",
    "\n",
    "Since a high temprature flattens the distribution, it tends to say things that make less sense together (since unlikely tokens are more likely to be sampled) for example I got the following: \n",
    "\n",
    "> Temperature: Once upon a time, there was a dark delicious pit held pumpkin still in Judaism, giving decorations in a royal participation one service hero path. Meanwhile unleashed shrines of even examination demons and vexes turned diabetes addicts restless vulnerable instead of officially beautiful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature and Top-k: Once upon a time, there was a woman whom she thought was not a man: the beautiful lady, the lovely one. The other of these men was not one but a woman. I was very proud to know who this lady was and\n",
      "Temperature and Top-p: Once upon a time, there was a remnant of loyalty of Indian workers who would not follow Congress. For two reasons, they fear retaliation and bitterness.\n",
      "\n",
      "The election has caused a lot of pressure on some, including high-ranking bureaucrats\n"
     ]
    }
   ],
   "source": [
    "# often times you will see temprature and top p or top k combined so that we remove all unlikely next tokens and\n",
    "# make some of the somewhat likely tokens more likely to be sampled\n",
    "# try playing around with the temprature and p and k and see how good of an output you can get!\n",
    "\n",
    "# Generate n tokens using different sampling strategies\n",
    "n_tokens = 40\n",
    "\n",
    "# Prepare input\n",
    "text = \"Once upon a time, there was a\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "p = 0.8\n",
    "k = 20\n",
    "temperature = 1.5\n",
    "\n",
    "\n",
    "def temp_top_k(x):\n",
    "    return sample_from_logits(\n",
    "        temperature_sampling(top_k_sampling(x, k=k), temperature=temperature)\n",
    "    )\n",
    "\n",
    "\n",
    "def temp_top_p(x):\n",
    "    return sample_from_logits(\n",
    "        temperature_sampling(top_p_sampling(x, p=p), temperature=temperature)\n",
    "    )\n",
    "\n",
    "\n",
    "temp_top_p_output = generate_n_tokens(input_ids, n_tokens, temp_top_p)\n",
    "temp_top_k_output = generate_n_tokens(input_ids, n_tokens, temp_top_k)\n",
    "\n",
    "# Decode outputs\n",
    "print(\n",
    "    \"Temperature and Top-k:\",\n",
    "    tokenizer.decode(temp_top_k_output[0], clean_up_tokenization_spaces=True),\n",
    ")\n",
    "print(\n",
    "    \"Temperature and Top-p:\",\n",
    "    tokenizer.decode(temp_top_p_output[0], clean_up_tokenization_spaces=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
